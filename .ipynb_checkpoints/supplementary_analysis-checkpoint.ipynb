{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e9dc88",
   "metadata": {},
   "source": [
    "# Supplementary Analysis: Interaction Effects (3-Way ANOVA)\n",
    "\n",
    "This notebook executes a 3-way ANOVA to analyze the interaction effects of **Sentiment** (Positive/Neutral/Negative), **Agent** (e.g., Copilot, Devin), and **User Type** (Bot/User) on the efficiency of pull requests (measured by iterations per hour).\n",
    "\n",
    "**Hypothesis**: There are significant interaction effects between the coding agent used, the sentiment of the review feedback, and the type of reviewer (human vs. bot).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9bc0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from huggingface_hub import hf_hub_download\n",
    "import traceback\n",
    "\n",
    "# Set plot style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Ensure VADER lexicon is downloaded\n",
    "try:\n",
    "    nltk.data.find('sentiment/vader_lexicon.zip')\n",
    "except LookupError:\n",
    "    print(\"Downloading VADER lexicon...\")\n",
    "    nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87576c60",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "We load the necessary datasets from the Hugging Face repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be0576d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Loading parquet files into dataframes...\n",
      "✓ All datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\")\n",
    "\n",
    "try:\n",
    "    # Download files from Hugging Face to local cache\n",
    "    dataset_name = \"hao-li/AIDev\"\n",
    "    \n",
    "    # Download all required files\n",
    "    files_to_download = [\n",
    "        \"pull_request.parquet\",\n",
    "        \"pr_reviews.parquet\",\n",
    "        \"pr_timeline.parquet\",\n",
    "    ]\n",
    "    \n",
    "    file_paths = {}\n",
    "    for filename in files_to_download:\n",
    "        # print(f\"  Downloading {filename}...\")\n",
    "        file_paths[filename] = hf_hub_download(repo_id=dataset_name, filename=filename, repo_type=\"dataset\")\n",
    "    \n",
    "    print(\"Loading parquet files into dataframes...\")\n",
    "    \n",
    "    # Basic PR data\n",
    "    pr_df = pd.read_parquet(file_paths[\"pull_request.parquet\"])\n",
    "    \n",
    "    # Reviews\n",
    "    pr_reviews_df = pd.read_parquet(file_paths[\"pr_reviews.parquet\"])\n",
    "    \n",
    "    # Events/Timeline\n",
    "    pr_timeline_df = pd.read_parquet(file_paths[\"pr_timeline.parquet\"])\n",
    "    \n",
    "    print(\"✓ All datasets loaded successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading datasets: {e}\")\n",
    "    traceback.print_exc()\n",
    "    pr_df, pr_timeline_df, pr_reviews_df = None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c03799",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data\n",
    "We calculate the lifecycle of each PR in hours and the number of iteration cycles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4261194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing PR data...\n",
      "Processed 31284 closed PRs.\n"
     ]
    }
   ],
   "source": [
    "if pr_df is not None:\n",
    "    print(\"Preprocessing PR data...\")\n",
    "    # Filter for closed PRs\n",
    "    closed_prs = pr_df[pr_df[\"state\"] == \"closed\"].copy()\n",
    "    \n",
    "    # Convert timestamps\n",
    "    closed_prs[\"created_at\"] = pd.to_datetime(closed_prs[\"created_at\"])\n",
    "    closed_prs[\"closed_at\"] = pd.to_datetime(closed_prs[\"closed_at\"])\n",
    "    \n",
    "    # Calculate lifecycle in hours\n",
    "    closed_prs[\"lifecycle_hours\"] = (closed_prs[\"closed_at\"] - closed_prs[\"created_at\"]).dt.total_seconds() / 3600\n",
    "    \n",
    "    pr_lifecycle = closed_prs[[\"id\", \"state\", \"lifecycle_hours\"]]\n",
    "    print(f\"Processed {len(pr_lifecycle)} closed PRs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0582d3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating iteration cycles...\n",
      "Calculated iterations for 33596 PRs.\n"
     ]
    }
   ],
   "source": [
    "if pr_timeline_df is not None:\n",
    "    print(\"Calculating iteration cycles...\")\n",
    "    pr_iterations = (\n",
    "        pr_timeline_df\n",
    "        .groupby(\"pr_id\")\n",
    "        .size()\n",
    "        .reset_index(name=\"iteration_cycles\")\n",
    "    )\n",
    "    print(f\"Calculated iterations for {len(pr_iterations)} PRs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0761c82",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis\n",
    "We use VADER to analyze the sentiment of PR reviews, classifying them into Positive, Negative, or Neutral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ac0257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sentiment...\n",
      "Calculating VADER scores...\n",
      "Aggregating sentiment per PR...\n",
      "           id  mean_compound_score sentiment_category\n",
      "0  2760115428               0.0000            Neutral\n",
      "1  2766353261              -0.1779           Negative\n",
      "2  2768057346               0.4926           Positive\n",
      "3  2768057378               0.4199           Positive\n",
      "4  2768132850              -0.5267           Negative\n"
     ]
    }
   ],
   "source": [
    "if pr_reviews_df is not None:\n",
    "    print(\"Analyzing sentiment...\")\n",
    "    # Extract relevant columns and drop missing bodies\n",
    "    reviews = pr_reviews_df[[\"pr_id\", \"body\"]].dropna().copy()\n",
    "    \n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def get_compound_score(text):\n",
    "        return sia.polarity_scores(text)[\"compound\"]\n",
    "    \n",
    "    # Calculate sentiment for each review\n",
    "    print(\"Calculating VADER scores...\")\n",
    "    reviews[\"compound_score\"] = reviews[\"body\"].apply(get_compound_score)\n",
    "    \n",
    "    # Aggregate by PR (Mean score)\n",
    "    print(\"Aggregating sentiment per PR...\")\n",
    "    pr_sentiment = reviews.groupby(\"pr_id\")[\"compound_score\"].mean().reset_index()\n",
    "    \n",
    "    # Classify sentiment\n",
    "    def classify_sentiment(score):\n",
    "        if score > 0.05:\n",
    "            return \"Positive\"\n",
    "        elif score < -0.05:\n",
    "            return \"Negative\"\n",
    "        else:\n",
    "            return \"Neutral\"\n",
    "            \n",
    "    pr_sentiment[\"sentiment_category\"] = pr_sentiment[\"compound_score\"].apply(classify_sentiment)\n",
    "    pr_sentiment.columns = [\"id\", \"mean_compound_score\", \"sentiment_category\"]\n",
    "    \n",
    "    print(pr_sentiment.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0465c2",
   "metadata": {},
   "source": [
    "## 4. Construct Final Dataset\n",
    "We merge the lifecycle, iterations, sentiment, and agent/user information into a single DataFrame for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ab57955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging data...\n",
      "Final merged dataset ready.\n",
      "           id   state  lifecycle_hours     pr_id_x  iteration_cycles  \\\n",
      "0  3265709660  closed         0.635556  3265709660                11   \n",
      "1  3214555104  closed        47.635833  3214555104                30   \n",
      "2  3214724259  closed         0.004444  3214724259                12   \n",
      "3  3214876564  closed         0.938333  3214876564                30   \n",
      "4  3215868710  closed        13.475000  3215868710                30   \n",
      "\n",
      "   mean_compound_score sentiment_category        agent     pr_id_y user_type  \\\n",
      "0             0.790600           Positive  Claude_Code  3265709660       Bot   \n",
      "1             0.332200           Positive  Claude_Code  3214555104      User   \n",
      "2             0.653100           Positive  Claude_Code  3214724259       Bot   \n",
      "3             0.499460           Positive  Claude_Code  3214876564       Bot   \n",
      "4             0.398367           Positive  Claude_Code  3215868710       Bot   \n",
      "\n",
      "   iteration_per_hour  \n",
      "0           17.307692  \n",
      "1            0.629778  \n",
      "2         2700.000000  \n",
      "3           31.971581  \n",
      "4            2.226345  \n"
     ]
    }
   ],
   "source": [
    "if pr_df is not None and pr_timeline_df is not None and pr_reviews_df is not None:\n",
    "    print(\"Merging data...\")\n",
    "    \n",
    "    # 1. Merge Lifecycle and Iterations\n",
    "    merged_df = pd.merge(\n",
    "        pr_lifecycle,\n",
    "        pr_iterations,\n",
    "        left_on=\"id\",\n",
    "        right_on=\"pr_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # 2. Merge Sentiment\n",
    "    # We use inner join to keep only PRs that have reviews/sentiment data\n",
    "    final_df = pd.merge(\n",
    "        merged_df,\n",
    "        pr_sentiment,\n",
    "        on=\"id\",\n",
    "        how=\"inner\" \n",
    "    )\n",
    "    \n",
    "    # 3. Add Agent info (from closed_prs)\n",
    "    # Note: 'agent' column is assumed to exist in the loaded pull_request.parquet\n",
    "    merged_final_df = final_df.merge(\n",
    "        closed_prs[[\"id\", \"agent\"]], \n",
    "        left_on=\"id\", \n",
    "        right_on=\"id\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 4. Add User Type info (from pr_reviews_df)\n",
    "    # Note: merging on pr_id and user_type, dropping duplicates to avoid explosion if multiple types exist\n",
    "    formatted_reviews = pr_reviews_df[[\"pr_id\", \"user_type\"]].drop_duplicates()\n",
    "    \n",
    "    merged_final_df = merged_final_df.merge(\n",
    "        formatted_reviews, \n",
    "        left_on=\"id\", \n",
    "        right_on=\"pr_id\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Calculate Efficiency Metric\n",
    "    merged_final_df[\"iteration_per_hour\"] = merged_final_df[\"iteration_cycles\"] / merged_final_df[\"lifecycle_hours\"]\n",
    "\n",
    "    print(\"Final merged dataset ready.\")\n",
    "    print(merged_final_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb5415",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis (3-Way ANOVA)\n",
    "We perform a 3-way ANOVA to test the effects of Agent, User Type, and Sentiment Category on `iteration_per_hour`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef726d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>df</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C(agent)</th>\n",
       "      <td>1.608584e+07</td>\n",
       "      <td>4.0</td>\n",
       "      <td>35.717816</td>\n",
       "      <td>1.591521e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C(user_type)</th>\n",
       "      <td>3.790535e+06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.666794</td>\n",
       "      <td>6.887059e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C(sentiment_category)</th>\n",
       "      <td>5.125198e+05</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.276050</td>\n",
       "      <td>1.027808e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C(agent):C(user_type)</th>\n",
       "      <td>5.487238e+06</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.184144</td>\n",
       "      <td>7.278689e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C(agent):C(sentiment_category)</th>\n",
       "      <td>6.204049e+06</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.887895</td>\n",
       "      <td>4.677021e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C(user_type):C(sentiment_category)</th>\n",
       "      <td>1.030431e+06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.576046</td>\n",
       "      <td>1.033277e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C(agent):C(user_type):C(sentiment_category)</th>\n",
       "      <td>2.178340e+06</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.418449</td>\n",
       "      <td>1.321321e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>6.522323e+08</td>\n",
       "      <td>5793.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   sum_sq      df          F  \\\n",
       "C(agent)                                     1.608584e+07     4.0  35.717816   \n",
       "C(user_type)                                 3.790535e+06     1.0  33.666794   \n",
       "C(sentiment_category)                        5.125198e+05     2.0   2.276050   \n",
       "C(agent):C(user_type)                        5.487238e+06     4.0  12.184144   \n",
       "C(agent):C(sentiment_category)               6.204049e+06     8.0   6.887895   \n",
       "C(user_type):C(sentiment_category)           1.030431e+06     2.0   4.576046   \n",
       "C(agent):C(user_type):C(sentiment_category)  2.178340e+06     8.0   2.418449   \n",
       "Residual                                     6.522323e+08  5793.0        NaN   \n",
       "\n",
       "                                                   PR(>F)  \n",
       "C(agent)                                     1.591521e-29  \n",
       "C(user_type)                                 6.887059e-09  \n",
       "C(sentiment_category)                        1.027808e-01  \n",
       "C(agent):C(user_type)                        7.278689e-10  \n",
       "C(agent):C(sentiment_category)               4.677021e-09  \n",
       "C(user_type):C(sentiment_category)           1.033277e-02  \n",
       "C(agent):C(user_type):C(sentiment_category)  1.321321e-02  \n",
       "Residual                                              NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model = ols('iteration_per_hour ~ C(agent) * C(user_type) * C(sentiment_category)', data=merged_final_df).fit()\n",
    "\n",
    "# Perform ANOVA\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Display results\n",
    "anova_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e57a75",
   "metadata": {},
   "source": [
    "## 6. Post-hoc Analysis (Tukey HSD)\n",
    "We perform Tukey HSD tests to determine which specific groups differ significantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86331486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tukey HSD: Agent ---\n",
      "        Multiple Comparison of Means - Tukey HSD, FWER=0.05        \n",
      "===================================================================\n",
      "   group1      group2     meandiff p-adj    lower    upper   reject\n",
      "-------------------------------------------------------------------\n",
      "Claude_Code      Copilot  -67.0081 0.1149 -143.1407   9.1244  False\n",
      "Claude_Code       Cursor  118.8639 0.0008   36.3567 201.3711   True\n",
      "Claude_Code        Devin  -49.4583 0.4148 -127.4152  28.4986  False\n",
      "Claude_Code OpenAI_Codex   33.2762 0.7535  -42.6197 109.1722  False\n",
      "    Copilot       Cursor   185.872    0.0  141.9737 229.7703   True\n",
      "    Copilot        Devin   17.5498 0.6379  -17.0464   52.146  False\n",
      "    Copilot OpenAI_Codex  100.2843    0.0   70.6226 129.9461   True\n",
      "     Cursor        Devin -168.3222    0.0 -215.3134 -121.331   True\n",
      "     Cursor OpenAI_Codex  -85.5877    0.0 -129.0743  -42.101   True\n",
      "      Devin OpenAI_Codex   82.7346    0.0   48.6621  116.807   True\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "--- Tukey HSD: User Type ---\n",
      " Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "=====================================================\n",
      "group1 group2 meandiff p-adj  lower    upper   reject\n",
      "-----------------------------------------------------\n",
      "   Bot   User  -75.104   0.0 -92.8166 -57.3914   True\n",
      "-----------------------------------------------------\n",
      "\n",
      "--- Tukey HSD: Sentiment Category ---\n",
      "   Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n",
      "=========================================================\n",
      " group1   group2  meandiff p-adj   lower    upper  reject\n",
      "---------------------------------------------------------\n",
      "Negative  Neutral -20.6843 0.3315 -54.8731 13.5046  False\n",
      "Negative Positive  -6.2283 0.8612 -34.2659 21.8092  False\n",
      " Neutral Positive  14.4559 0.4301 -12.8941  41.806  False\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Compare agents\n",
    "print(\"--- Tukey HSD: Agent ---\")\n",
    "tukey_agent = pairwise_tukeyhsd(endog=merged_final_df['iteration_per_hour'], groups=merged_final_df['agent'], alpha=0.05)\n",
    "print(tukey_agent)\n",
    "\n",
    "# Compare user types\n",
    "print(\"\\n--- Tukey HSD: User Type ---\")\n",
    "tukey_user = pairwise_tukeyhsd(endog=merged_final_df['iteration_per_hour'], groups=merged_final_df['user_type'], alpha=0.05)\n",
    "print(tukey_user)\n",
    "\n",
    "# Compare sentiment categories\n",
    "print(\"\\n--- Tukey HSD: Sentiment Category ---\")\n",
    "tukey_sentiment = pairwise_tukeyhsd(endog=merged_final_df['iteration_per_hour'], groups=merged_final_df['sentiment_category'], alpha=0.05)\n",
    "print(tukey_sentiment)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
